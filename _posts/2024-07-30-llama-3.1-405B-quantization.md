---
layout: post
title: "Llama 3.1 405B"
date: 2024-07-30
tags: [llm, llama, self-hosting]
---

Simon Willison, creator of Datasette and co-creator of Django, recently [asked on Twitter](https://x.com/simonw/status/1817791263590228184) for a "vibe check" on Llama 3.1 405B. He was particularly interested in whether it's becoming a credible self-hosted alternative to the best OpenAI or Anthropic models, and if any companies previously hesitant about sending data to API providers are now using it.

In response to a comment about fp16 being superior to other quantizations, with Hyperbolic Labs being mentioned as the only provider offering it, I noted:

"AWS and Nvidia NGC are not explicit about bf16, but results appear superior to what's dubbed 'Llama 3.1 405B Turbo' (quantized and perhaps otherwise meddled with)."

The fact that other hosters are potentially offering quantized versions without explicitly stating so adds an interesting dimension to the discussion. It suggests that the landscape of LLM hosting and deployment is still evolving, with different providers experimenting with various techniques to optimize performance.