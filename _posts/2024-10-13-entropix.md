---
layout: post
title: "Entropy-based \"Shrek Sampling\""
author: "Nils Durner"
categories: [journal]
tags: [entropix, shrek, shrek sampling, entropy sampling, llm, llama, mlx, slm]
date: 2024-10-13
last_updated: 2024-10-13
---

The community is porting the "Shrek Sampler" to different hardware (MLX) and Transformer architectures, just days after [Entropix](https://github.com/xjdr-alt/entropix) was first released. Nice visualization from the MLX port: [9.9 vs. 9.11](https://x.com/sam_e_farrar/status/1844791813913083998). Pierre-Carl Langlais, Co-founder of Pleias.fr, posted his Colab notebook that runs entropix with Smollm-360M (the original release uses Llama 3.2 1B). He kindly answered my question if the [entropix hype is real](https://www.linkedin.com/feed/update/urn:li:activity:7250786301011017728?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7250786301011017728%2C7250808642621304832%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287250808642621304832%2Curn%3Ali%3Aactivity%3A7250786301011017728%29):
> Definitely needs more testing but better stabilization of tiny models output while not sacrificing their creativity is certainly the way. Long term objective is efficient reasoner able to parse and reformulate external information with minimum compute cost.