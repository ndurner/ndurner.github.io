---
layout: post
title: "LLM on smartphone"
date: 2024-04-09
last_updated: 2024-04-09
description: "Demonstrates running LLM inference on smartphones, comparing on-device performance, energy usage, and responsiveness for mobile AI assistants."
tags: [llm, slm, smartphone, llamacpp, gguf, quantization, layla]
---

Following up on [our conversation](https://www.linkedin.com/feed/update/urn:li:activity:7183003138419744768?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7183003138419744768%2C7183008781155393536%29&replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7183003138419744768%2C7183063800957259776%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287183008781155393536%2Curn%3Ali%3Aactivity%3A7183003138419744768%29&dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287183063800957259776%2Curn%3Ali%3Aactivity%3A7183003138419744768%29) on quantized models on smartphones, Stefano Fiorucci wrote a post about how to run a small language model on a smartphone: [](https://www.linkedin.com/posts/stefano-fiorucci_llm-genai-edgecomputing-activity-7183365537618411520-PU2s?utm_source=share&utm_medium=member_desktop).

This involves either Layla Lite App or Temux. One commenter recommended [LLM Farm](https://llmfarm.site/) on iPhone.