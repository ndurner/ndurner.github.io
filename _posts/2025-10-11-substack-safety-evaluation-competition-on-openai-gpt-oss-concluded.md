---
title: "Safety evaluation competition on OpenAI gpt-oss concluded"
date: 2025-10-11
last_updated: 2025-10-11T00:00:00+00:00
author: "Nils Durner"
redirect_to:
  - https://ndurner.substack.com/p/kaggle-red-teaming-challenge-openai-gpt-oss-concluded
tags: [Substack]
substack_url: "https://ndurner.substack.com/p/kaggle-red-teaming-challenge-openai-gpt-oss-concluded"
canonical_url: "https://ndurner.substack.com/p/kaggle-red-teaming-challenge-openai-gpt-oss-concluded"
sitemap: false
---

Published on Substack.

The Kaggle safety evaluation “red-teaming” challenge on OpenAI gpt-oss has concluded with a workshop symposium this week. The symposium opened with talks from D. Sculley, our host and OpenAI researcher focused on responsible and reliable ML, and Samuel Marks, an AI safety researcher at Anthropic. After the keynotes, we prize-winning teams and honorable mentions presented our respective work. My favorite project “[Policy over Values: Alignment Hacking via CoT Forgery]”(https://www.kaggle.com/comp...

[Read on Substack](https://ndurner.substack.com/p/kaggle-red-teaming-challenge-openai-gpt-oss-concluded).
